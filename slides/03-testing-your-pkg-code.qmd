---
title: "Ensuring your code works as expected - and introduction to testing"
format: 
  revealjs:
    slide-number: true
    slide-level: 4
editor: source
bibliography: references.bib
---

## Testability

Testability is defined as the degree to which a system 
or component facilitates the establishment of test objectives 
and the execution of tests to determine 
whether those objectives have been achieved.

In order to be successful, 
a test needs to be able to execute the code you wish to test, 
in a way that can trigger a defect 
that will propagate an incorrect result to a program point
where it can be checked against the expected behaviour. 

## High-level properties for effective test writing and execution

**controllability**: the code under test needs to be able to be programmatically controlled

**observability**: the outcome of the code under test needs to be able to be verified

**isolateablilty**: the code under test needs to be able to be validated on its own

**automatability**: the tests should be able to be executed automatically

Source: [CPSC 310](https://github.com/ubccpsc/310/blob/master/resources/readings/TestabilityAssertions.md) 
& [CPSC 410](https://www.cs.ubc.ca/~rtholmes/teaching/2015t1/cpsc410/slides/410_19_testability.pdf) class notes from Reid Holmes, UBC]

## What kinds of tests do we write for our functions?

When I am designing tests for my function, 
I like to think about three broad categories of tests, 
and then write 2-3 tests for each 
(or more if the function is complex and takes many arguments):

1. Simple expected use cases

2. Edge cases (unexpected, or rare use cases)

3. Errors

We will come back to these and provide specific examples in a few minutes.

## When do we write tests? 

- Anytime you think about writing a function!

- You can even write your tests before you write your function, 
you just have to have planned what function inputs and outputs to expect.

- Writing your tests before implementing your function 
can even improve your productivity [@erdogmus2005effectiveness]! 

## Workflow for writing functions and tests

1. Write the function specifications and documentation - 
but do not implement the function.

2. Plan the test cases and document them.

3. Create test data that is useful for assessing whether your function works as expected.

4. Write the tests to evaluate your function based on the planned test cases and test data.

5. Implement the function by writing the needed code in the function body to pass the tests.

6. Iterate between steps 2-5 to improve the test coverage and function.

## Example of workflow for writing functions and tests for data science

Let's pretend we haven't yet written our `count_classes` function, 
and follow the workflow I just outlined to develop our function 
and it's test suite.

### 1. Write the function specifications and documentation - but do not implement the function

The first thing we should do is write the function specifications and documentation. This can effectively represented by an empty function and roxygen2-styled documentation in R as shown below:

```{r}
#| eval: false

#' Count class observations
#'
#' Creates a new data frame with two columns, 
#' listing the classes present in the input data frame,
#' and the number of observations for each class.
#'
#' @param data_frame A data frame or data frame extension (e.g. a tibble).
#' @param class_col unquoted column name of column containing class labels
#'
#' @return A data frame with two columns. 
#'   The first column (named class) lists the classes from the input data frame.
#'   The second column (named count) lists the number of observations for each class from the input data frame.
#'   It will have one row for each class present in input data frame.
#'
#' @export
#'
#' @examples
#' count_classes(mtcars, am)
count_classes <- function(data_frame, class_col) {
  # returns a data frame with two columns: class and count
}
```

### 2. Plan the test cases and document them

Next, we should plan out our test cases and start to document them. 

At this point we can sketch out a skeleton for our test cases with code 
but we are not yet ready to write them, 
as we first will need to reproducibly create test data 
that is useful for assessing whether your function works as expected. 

### 2. Plan the test cases and document them (cont'd)

So considering our function specifications, 
some kinds of input we might anticipate our function may receive, 
and correspondingly what it should return is listed in the following tables:

| Test category | Function input | Expected function output |
|---------------|----------------|--------------------------|
| Simple expected use cases | Dataframe with 2 classes, with 2 observations per class, and an unquoted column name containing class labels. Data frame also has another column a numeric values for each observation. | A new data frame with two columns and two rows. One column with the class names, and one with the counts of 2 observations for each class. |
| Simple expected use cases | Dataframe with 2 classes, with 2 observations for one class, 1 observation for another class, and an unquoted column name containing class labels. Data frame also has another column a numeric values for each observation. | A new data frame with two columns and two rows. One column with the class names, and one with the counts of 2 for one class and 1 for the other class. |

### 2. Plan the test cases and document them (cont'd)

| Test category | Function input | Expected function output |
|---------------|----------------|--------------------------|
| Edge cases | Data frame with one class, with 3 observations per class, and an unquoted column name of column containing class labels. Data frame also has another column a numeric values for each observation.
| Edge cases | A data frame with three two columns (`class_labels` and `values`) but observation (e.g., it is an empty data frame). Also an unquoted column name of column containing class labels. | A new empty data frame with two columns and 0 rows. One column with the class names, and one with the counts of observations for each class. |

### 2. Plan the test cases and document them (cont'd)

| Test category | Function input | Expected function output |
|---------------|----------------|--------------------------|
| Errors | Data frame with two classes, with two observations per class and a character vector of class labels as a separate object | An error that reports the value for the class_col argument should be an unquoted column name of column containing class labels from the data frame given in to the first argument. |
| Errors | A list as the value for the argument to `data_frame`, and the unquoted name of one of the list elements as the value for the argument `class_col` | An error that reports the value for the the `data_frame` object should be a data frame or data frame extension (e.g. a tibble). |

### 2. Plan the test cases and document them (cont'd)

Next, I sketch out a skeleton for the unit tests. 
For R, we will use the well maintained 
and popular [`testthat`](https://testthat.r-lib.org/) R package 
for writing our tests. 

With `testthat` we create a `test_that` statement 
for each related group of tests for a function. 
For our example, we will create the four `test_that` statements shown below:

```{r}
#| eval: false
test_that("`count_classes` should return a data frame, or data frame extension, 
with the number of rows that corresponds to the number of unique classes 
in the column passed to `class_col`, 
and whose values in the `count` column correspond to the number of observations 
for the group in the `class` column from the original data frame", {
  # "expected use cases" tests to be added here
})

test_that("`count_classes` should return an empty data frame, or data frame extension, 
if the input to the function is an empty data frame", {
  # "edge cases" test to be added here
})

test_that("`count_classes` should throw an error when incorrect types 
are passed to `data_frame` and `class_col` arguments", {
  # "error" tests to be added here
})
```

### 3. Create test data that is useful for assessing whether your function works as expected

Now that we have a plan, we can create reproducible test data for that plan! When we do this, we want to keep our data as small and tractable as possible. We want to test things we know the answer to, or can at a minimum calculate by hand. We will use R code to reproducibly create the test data. We will need to do this for the data we will feed in as inputs to our function in the tests, as well as the data we expect our function to return.

```{r}
#| eval: false

# function input for tests
set.seed(2024)
two_clases_two_obs <- data.frame(class_labels = rep(c("class1", "class2", 2)),
                                 values = runif(4))
two_classes_2_and_1_obs <- data.frame(class_labels = c("class1", "class1", "class2"),
                                      values = runif(3))
one_class_2_obs <- data.frame(class_labels = c("class1", "class1"),
                                      values = runif(2))
empty_df  <- data.frame(class_labels = character(0),
                        values = double(0))
vector_class_labels <- rep(c("class1", "class2"), 2)
two_classes_two_obs_as_list <- list(class_labels = rep(c("class1", "class2"), 2),
                                    values = runif(4))
```

### 3. Create test data that is useful for assessing whether your function works as expected (cont'd)

```{r}
#| eval: false

# expected function output
two_clases_two_obs_output <- data.frame(class = c("class1", "class2"),
                                        count = c(2,2))
two_classes_2_and_1_obs_output <- data.frame(class = c("class1", "class2"),
                                      count = c(2, 1))
one_class_2_obs_output <- data.frame(class = "class1",
                              count = 2)
empty_df_output <- data.frame(class = character(0),
                              count = numeric(0))
```

### 4. Write the tests to evaluate your function based on the planned test cases and test data

Now we are ready write the internals for our tests! 
We will do this by using `expect_*` functions from the testthat package. 

**`testthat` pacakge test structure:**

```{r}
#| eval: false

test_that("Message to print if test fails", expect_*(...))
```

### 4. Write the tests to evaluate your function based on the planned test cases and test data (cont'd)

#### Common expect_* statements for use with `test_that`:

Is the object equal to a value?

- `expect_identical` - test two objects for being exactly equal
- `expect_equal` - compare R objects x and y testing ‘near equality’ (can set a tolerance)
- `expect_equivalent` - compare R objects x and y testing ‘near equality’ (can set a tolerance) and does not assess attributes

Does code produce an output/message/warning/error?

- `expect_error` - tests if an expression throws an error
- `expect_warning` - tests whether an expression outputs a warning
- `expect_output` - tests that print output matches a specified value

Is the object true/false?

These are fall-back expectations that you can use when none of the other more specific expectations apply. The disadvantage is that you may get a less informative error message.

- `expect_true` - tests if the object returns TRUE
- `expect_false` - tests if the object returns FALSE

### 4. Write the tests to evaluate your function based on the planned test cases and test data (cont'd)

```{r}
#| eval: false
test_that("`count_classes` should return a data frame, or data frame extension, 
with the number of rows that corresponds to the number of unique classes 
in the column passed to `class_col`, 
and whose values in the `count` column correspond to the number of observations 
for the group in the `class` column from the original data frame", {
    expect_s3_class(count_classes(two_classes_2_obs, class_lables), "tibble")
    expect_equivalent(count_classes(two_classes_2_obs, class_lables), 
                      two_classes_2_obs_output)
    expect_equivalent(count_classes(two_classes_2_and_1_obs, class_lables),
                      two_classes_2_and_1_obs_output)
})
```

### 4. Write the tests to evaluate your function based on the planned test cases and test data (cont'd)

```{r}
test_that("`count_classes` should return an empty data frame, 
or data frame extension, if the input to the function is an empty data frame", {
    expect_equivalent(count_classes(one_class_2_obs, class_lables), 
                      one_class_2_obs_output)
    expect_equivalent(count_classes(empty_df, class_lables), empty_df_output)
})
```

### 4. Write the tests to evaluate your function based on the planned test cases and test data (cont'd)

```{r}
test_that("`count_classes` should throw an error when incorrect types 
are passed to `data_frame` and `class_col` arguments", {
    expect_error(count_classes(two_classes_2_obs, vector_class_labels))
    expect_error(count_classes(two_classes_two_obs_as_list, class_lables))
})
```

### Wait what??? Most of our tests fail…

Yes, we expect that, we haven’t written our function body yet!

### 5. Implement the function by writing the needed code in the function body to pass the tests

FINALLY!! We can write the function body for our function! And then call our tests to see if they pass!

```{r}
#| eval: false

#' Count class observations
#'
#' Creates a new data frame with two columns, 
#' listing the classes present in the input data frame,
#' and the number of observations for each class.
#'
#' @param data_frame A data frame or data frame extension (e.g. a tibble).
#' @param class_col unquoted column name of column containing class labels
#'
#' @return A data frame with two columns. 
#'   The first column (named class) lists the classes from the input data frame.
#'   The second column (named count) lists the number of observations for each class from the input data frame.
#'   It will have one row for each class present in input data frame.
#'
#' @export
#'
#' @examples
#' count_classes(mtcars, am)
count_classes <- function(data_frame, class_col) {
    if (!is.data.frame(data_frame)) {
        stop("`data_frame` should be a data frame or data frame extension (e.g. a tibble)")
    }
    data_frame |>
        dplyr::group_by({{ class_col }}) |>
        dplyr::summarize(count = dplyr::n()) |>
        dplyr::rename("class" = {{ class_col }})
}
```

### 6. Iterate between steps 2-5 to improve the test coverage and function

Are we done? For the purposes of this demo, yes! However in practice you would usually cycle through steps 2-5 two-three more times to further improve our tests and and function

## Where do the function and test files go?

- The test code goes in a folder called `tests`

- To setup the correct file and directory structure for our tests, we run:

```{r}
#| eval: false

# The 
usethis::use_testthat()
```

Which will:

- Create a `tests/testthat/` directory, where we will put our test files
- Add `testthat` to the `Suggests` field in the `DESCRIPTION` file.
- Create a file `tests/testthat.R` that runs all your tests when `R CMD` check runs 

## Where do the function and test files go?

- As you define functions in your package, in the files below `R/`, you add the corresponding tests to `.R` files in `tests/testthat/`

-  We strongly recommend that the organisation of test files match the organisation of `R/` files, for example, for the EDA pacakge:

```
R                                           tests/testthat
└── count_classes.R                         └── test-count_classes.R
    count_classes <- function(...) {...}          test_that("count_classes does this", {...})
                                                  test_that("count_classes does that", {...})
```

## How do we run the test suite?

```
devtools::test()
```

## Attributions:

- [*R Packages (2e)*](https://r-pkgs.org/) by Hadley Wickham and Jennifer Bryan

## References